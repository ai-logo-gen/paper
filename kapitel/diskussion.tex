\newpage
\section{Discussion}\label{sec:diskussion}

This study systematically investigated the resource-efficient specialization of a multimodal diffusion model for minimalist logo generation. By combining LoRA-based fine-tuning with ControlNet guidance, a prototype was developed that effectively processes both textual and structural constraints. This chapter interprets the experimental findings by directly addressing the research hypotheses formulated in Chapter \ref{sec:introduction} and discusses limitations alongside future research directions.

\subsection{Interpretation of Results and Hypothesis Confirmation}

The experimental results demonstrate the efficacy of the proposed hybrid approach in achieving professional-grade design automation with limited resources. This section synthesizes the findings to confirm the research hypotheses.

\subsubsection*{Efficacy of the Hybrid Strategy (H1 \& H2)}
The combination of ControlNet and LoRA fine-tuning proved highly effective, confirming the fundamental hypotheses regarding structure and quality.
\textbf{Hypothesis H1}, stating that sketch conditioning leads to significantly higher structural correspondence without loss of image quality, was clearly confirmed. Evaluation showed a dramatic increase in the SSIM score from 0.621 for the base model without ControlNet to 0.819 with ControlNet (+31.9\%), while maintaining overall aesthetic markers. Although the FID score on the test set showed a slight fluctuation from 223.3 to 228.3 (+2.2\%), this marginal change is considered non-significant in the context of the substantial structural gain, thereby supporting the hypothesis. This validates the theoretical considerations of \citet{ZHANG2023}, positioning ControlNet as a solution for the geometric limitations of pure text-to-image models.

Simultaneously, \textbf{Hypothesis H2}, postulating quality improvement through domain-specific fine-tuning, was fully confirmed. The best fine-tuned model achieved a CLIP score of 0.287 vs. 0.277 for the base model (+3.6\%) and an FID reduction from 228.3 to 157.6 (-31.0\%). These results align with \citet{ruiz2023dreamboothfinetuningtexttoimage}, validating significant gains even with limited iterations. The efficiency of this combination is particularly noteworthy, as significant quality improvements were achieved with a compact dataset and limited resources. Notably, the slight SSIM drop (0.819 to 0.788) reflects a desired tradeoff: shifting focus from rigid structure to semantic accuracy and visual quality suitable for minimalist logos.

\subsubsection*{Hyperparameter Dynamics (H3)}
The systematic analysis confirmed \textbf{Hypothesis H3}, identifying the learning rate as the dominant factor for both semantic coherence (CLIP) and visual quality (FID). A high learning rate of $1e-4$ combined with the ``extended'' configuration yielded the best overall results. Higher LoRA ranks (32) tended to enable better semantic coherence at high learning rates, consistent with \citet{HU2021}, suggesting that higher ranks approach full fine-tuning capacity. The findings highlight the importance of careful hyperparameter tuning for LoRA, where a high learning rate combined with extended module adaptation provides the best balance between plasticity and stability. However, it is essential to emphasize that these observations are specific to our experimental design and should not be generalized without further validation on different architectures or datasets.

\subsubsection*{Qualitative Assessment and Practical Viability (H4)}
\textbf{Hypothesis H4}, suggesting the superiority of the optimized prototype in qualitative assessment, was supported by the stylistic analysis. The evaluation (see Section \ref{sec:qualitative_analyse}) indicated that the fine-tuned model better adheres to the minimalist design intent compared to the base model. Specifically, it demonstrated improved capabilities in generating solid backgrounds and harmonic color palettes suitable for professional branding. While the base model frequently introduced unwanted textures, the optimized prototype produced cleaner, vector-like aesthetics. This underscores the model's practical potential for bridging the gap between rough ideation and polished design, effectively handling authentic human inputs despite remaining challenges in fine-grained detail, such as precise text rendering and geometric sharpness.


\subsection{Limitations and Future Work}

While the results demonstrate the viability of the proposed pipeline, several limitations identify key areas for future optimization. A central challenge remains the consistent generation of perfect fine-grained details, such as razor-sharp geometric lines (e.g., in monograms) and highly legible typography. This constraint is primarily attributed to four factors:
\begin{enumerate}
    \item \textbf{Data Quantity:} The specialized training subset was relatively small (1,500 images) to accommodate iterative hyperparameter optimization.
    \item \textbf{Data Quality:} The semi-structured captions from the source dataset often lacked the descriptive depth required for the model to learn complex stylistic and geometric nuances.
    \item \textbf{Training Duration:} To maintain resource efficiency on consumer hardware, the number of training steps was kept relatively low, which may have limited the convergence of fine-grained structural features.
    \item \textbf{Synthetic Artifacts:} The generation of synthetic training data sometimes introduced unwanted lines or thematic elements (e.g., pencils or rough textures) not present in the original logo input. These artifacts occurred because the model associated the "sketch" domain with physical drawing tools, potentially polluting the structural control signal.
\end{enumerate}

Despite these limitations, the proposed method is recommended for productive use in the early stages of the design process, specifically for ideation and the rapid generation of stylized layout concepts. Future work will focus on the following pillars:

\subsubsection*{Data Quality and Augmentation}
Future iterations should employ Vision-Language Models (VLMs) to generate ultra-detailed captions and structured categorizations, such as distinguishing wordmarks from pictorial marks. This would significantly enhance the model's semantic understanding \cite{zeng-etal-2025-enhancing-large}. Furthermore, synthetic sketch generation could be diversified using more advanced edge detection algorithms or dedicated sketch models to improve robustness against the variability of human hand-drawn inputs. Crucially, future data pipelines should prioritize the logical traceability of synthetic signals, ensuring that generated sketches are strictly consistent with the original logo templates and free from non-semantic artifacts that do not reflect the underlying design.

\subsubsection*{Modeling Efficiency and Stability}
Implementation of QLoRA (4-bit quantization) \cite{dettmers2023qloraefficientfinetuningquantized} and ``torch.compile()'' would allow for larger batch sizes and faster training on consumer hardware. Furthermore, Bayesian optimization could replace grid search to more efficiently explore the hyperparameter space and identify regions of stable convergence, addressing the training instability sometimes observed in LoRA fine-tuning \cite{luo2024privacypreservinglowrankadaptationmembership}.

\subsubsection*{Evaluation Metrics}
As standard metrics like CLIP, FID, and SSIM do not fully capture design-specific criteria such as memorability or vectorizability, future work should focus on developing domain-specific evaluation frameworks. This includes automated vectorization success rates and larger-scale user studies with professional designers to provide more robust and practically relevant quality assessments.
