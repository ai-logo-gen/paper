\section{Discussion}\label{sec:diskussion}

This study systematically investigated the resource-efficient specialization of a multimodal diffusion model for minimalist logo generation. By combining LoRA-based fine-tuning with ControlNet guidance, a prototype was developed that effectively processes both textual and structural constraints. This chapter interprets the experimental findings by directly addressing the research hypotheses formulated in Chapter \ref{sec:introduction} and discusses limitations alongside future research directions.

\subsection{Interpretation of Results and Hypothesis Confirmation}

The experimental results demonstrate the efficacy of the proposed hybrid approach in achieving professional-grade design automation with limited resources. This section synthesizes the findings to confirm the research hypotheses.

\paragraph{Efficacy of the Hybrid Strategy (H1 \& H2)}
The combination of ControlNet and LoRA fine-tuning proved highly effective, confirming the fundamental hypotheses regarding structure and quality.
\textbf{Hypothesis H1}, stating that sketch conditioning leads to significantly higher structural correspondence, was clearly confirmed. Evaluation showed a dramatic increase in the SSIM score from 0.617 for the base model without ControlNet to 0.817 with ControlNet (+32.4\%). This validates the theoretical considerations of \textcite{ZHANG2023}, positioning ControlNet as a solution for the geometric limitations of pure text-to-image models.

Simultaneously, \textbf{Hypothesis H2}, postulating quality improvement through domain-specific fine-tuning, was fully confirmed. The best fine-tuned model achieved a CLIP score of 0.284 vs. 0.274 for the base model (+3.7\%) and an FID reduction from 229.0 to 158.2 (-30.9\%). These results align with \textcite{ruiz2023dreamboothfinetuningtexttoimage}, validating significant gains even with limited iterations. The efficiency of this combination is particularly noteworthy, as significant quality improvements were achieved with a compact dataset and limited resources. Notably, the slight SSIM drop (0.817 to 0.789) reflects a desired tradeoff: shifting focus from rigid structure to semantic accuracy and visual quality suitable for minimalist logos.

\paragraph{Hyperparameter Dynamics (H3)}
The systematic analysis confirmed \textbf{Hypothesis H3}, identifying the learning rate as the dominant factor for both semantic coherence (CLIP) and visual quality (FID). A high learning rate of $1e-4$ combined with the ``extended'' configuration yielded the best overall results. Higher LoRA ranks (32) tended to enable better semantic coherence at high learning rates, consistent with \textcite{HU2021}, suggesting that higher ranks approach full fine-tuning capacity. The findings highlight the importance of careful hyperparameter tuning for LoRA, where a high learning rate combined with extended module adaptation provides the best balance between plasticity and stability.

\paragraph{Qualitative Assessment and Practical Viability (H4)}
\textbf{Hypothesis H4}, suggesting the superiority of the optimized prototype in qualitative assessment, was supported by the stylistic analysis. The evaluation (see Section \ref{sec:qualitative_analyse}) indicated that the fine-tuned model better adheres to the minimalist design intent compared to the base model. Specifically, it demonstrated improved capabilities in generating solid backgrounds and harmonic color palettes suitable for professional branding. While the base model frequently introduced unwanted textures, the optimized prototype produced cleaner, vector-like aesthetics. This underscores the model's practical potential for bridging the gap between rough ideation and polished design, effectively handling authentic human inputs despite minor limitations in fine-grained detail.


\subsection{Limitations and Future Work}

While the results demonstrate the viability of the proposed pipeline, several limitations identify key areas for future optimization.

\subsubsection*{Data Quality and Augmentation}
The dataset, while extensive, is limited to specific minimalist styles. The existing captions are often simplistic tags, lacking descriptive depth.
\begin{itemize}
	\item \textbf{Future Work:} Future iterations should employ Vision-Language Models (VLMs) to generate ultra-detailed captions and structured categorizations (e.g., wordmarks vs. pictorial marks). This would significantly enhance the model's semantic understanding \parencite{zeng-etal-2025-enhancing-large}. Additionally, synthetic sketch generation could be diversified using edge detection or dedicated sketch models to improve robustness via data augmentation.
\end{itemize}

\subsubsection*{Modeling Efficiency and Stability}
The experiments revealed training instability characteristic of LoRA fine-tuning \parencite{luo2024privacypreservinglowrankadaptationmembership}. Hardware constraints (consumer GPU) limited batch sizes to 8.
\begin{itemize}
	\item \textbf{Future Work:} Implementation of QLoRA (4-bit quantization) \parencite{dettmers2023qloraefficientfinetuningquantized} and ``torch.compile()'' would allow for larger batch sizes and faster training on consumer hardware. Furthermore, Bayesian optimization could replace grid search to more efficiently explore the hyperparameter space and identify stable convergence regions.
\end{itemize}

\subsubsection*{Evaluation Metrics}
Standard metrics (CLIP, FID, SSIM) do not fully capture design-specific criteria like memorability or vectorizability.
\begin{itemize}
	\item \textbf{Future Work:} Developing domain-specific metrics (e.g., automated vectorization success rates) and conducting larger-scale studies with professional designers would provide more robust quality assessments.
\end{itemize}
